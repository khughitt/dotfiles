# tokenization (nltk)
nltk.tokenize.RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+')
tokenizer.tokenize(doc)

# lemmatization (nltk)
from nltk.stem.wordnet import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatizer.lemmatize(word)

# sentences
nltk.sent_tokenize(doc)

# stemmer comparison
# https://stackoverflow.com/a/24663617/554531

# vi:syntax=python
