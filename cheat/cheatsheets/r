# get a specific column from each list in a list-of-lists
lapply(lst, '[[', 'foo')

# specify factor level order
ordered(df$foo, levels = c("small", "med", "large"))

# PCA (column-wise)
pca <- prcomp(t(dat), scale = TRUE)

pca_dat <- pca$x[, 1:2]
colnames(pca_dat) <- c("PC1", "PC2")

# compute variance explained
var_explained <- round(summary(pca)$importance[2, 1:2] * 100, 2)

# add labels and plot
pca_dat <- pca_dat %>%
  as.data.frame() %>%
  rownames_to_column('row_label')

plt <- ggplot(pca_dat, aes(x = PC1, y = PC2)) +
  geom_point() +
  xlab(sprintf("PC1 (%.2f%% variance)", var_explained[1])) +
  ylab(sprintf("PC2 (%.2f%% variance)", var_explained[2]))

# UMAP
library(uwot)
umap(t(dat), n_neighbors = 15, n_components = 2, init = 'spectral', scale = FALSE,
     min_dist = 0.01)

# sweep (ex: cpm normalization)
dat <- sweep(dat, 2, colSums(dat), '/') * 1E6
dat <- sweep(dat, 2, colSums(dat, na.rm = TRUE), '/') * 1E6

# recursively update one list, using another
modifyList(a, b)

# read file as a string
readr::read_file('file.txt')

# json
jsonlite::read_json('input.json')
jsonlite::write_json('output.json')

# json - create a list of dicts
df = data.frame(from=c('a', 'b'), to=c('c', 'd'))
jsonlite::toJSON(df)

# or..
lst = list(list('from'='a', 'to'='b'), list('from'='c', 'to'='d'))
jsonlite::toJSON(lst)

# yaml
yaml::read_yaml(file)

# get filename / extension
tools::file_ext("file.txt")
tools::file_path_sans_ext("file.txt")

# today
strftime(Sys.time(), "%Y-%m-%d")

# iso 8601 date
strftime(as.POSIXlt(Sys.time(), "UTC"), "%Y-%m-%dT%H:%M:%S%z")

# color map from list of colors
colorRampPalette(c('red', 'blue'))(100)

# ColorBrewer palettes
RColorBrewer::brewer.pal(12, "Set3")

# generate all n-tuple combinations of vector elements
combn(vec, 2)

# get size of all pairwise intersections for each pair of vectors in a list
crossprod(table(stack(lst)))

# get classes of each column
sapply(df, class)

# convert dataframe to numeric matrix
# "data.matrix(df)" should work, but incorrectly converts double columns to integers in
# some cases..
x <- matrix(as.numeric(unlist(df)), nrow=nrow(df))
colnames(x) <- colnames(df)
rownames(x) <- rownames(df)

# convert a dataframe of factors to a numeric matrix
data.matrix(df)

# compute ranks for each column in a dataframe
do.call(cbind, lapply(df, rank))

# print stack trace
traceback()

# enter debugger for a specific function
debugonce(pkg:::foo)
debug(pkg:::foo)

# set breakpoint
utils::setBreakpoint('file.R', 99)

# print variable memory usage
# http://stackoverflow.com/questions/1395270/determining-memory-usage-of-objects
sort( sapply(ls(),function(x){object.size(get(x))}))

# aheatmap
aheatmap(dat, color = viridis(100), Colv = FALSE, labCol = '', annRow = annot_df)

# cosine similarity
# source: https://stats.stackexchange.com/a/367216/18331
mat <- as.matrix(df)
sim <- mat / sqrt(rowSums(mat * mat))
sim <- sim %*% t(sim)

# matrix inverse
solve(A)

# upper / lower triangular matrix
X[upper.tri(X, diag=FALSE)]
X[lower.tri(X, diag=FALSE)]

# imputation
missMethods::impute_median(mat)                  # median value (columns)
missMethods::impute_mean(mat, type = 'rowwise')  # mean value (rows)
as.matrix(VIM::kNN(mat)[, 1:ncol(mat)])          # k-NN

# write xlsx files (openxlsx does not depend on java like the xlsx package..)
tbls <- list(a=df1, b=df2, ...)
openxlsx::write.xlsx(tbls, file="out.xlsx")
