Basics
------

# rows
axis = 0

# columns
axis = 1

Series
------

# rename series
df.name = 'foo'

# convert dataframe to a series
pd.Series(df.values.ravel())

# convert a series to a dataframe
s.to_frame()

Selection
---------

# select single column as a series
df['col']

# select one or more columns as a dataframe
df[['col']]
df[['col1', 'col2']]

# get index as an integer
ind = df.index[df["col"] == val].tolist()[0]

# get upper triangular matrix indices
ind = np.triu_indices(5)         # incl. diagonal
ind = np.triu_indices(5, k=1)    # excl. diagonal

# extract values of indices as 1d array
df.values[ind]

# drop column
df.drop('col', axis=1)

# drop first column
df.iloc[: , 1:]

# drop index
df.reset_index(Drop=True)

# drop missing
df.dropna()

Iteration
---------

# iterate over rows as series
for index, row in df.iterrows():
    row.foo
    row[0]

# iterate over rows as dicts
df.to_dict(orient="records")

# iterate over columns
for cname, col in df.items():
  print(col)

# iterate over items in a series
for index, value in s.items():
  ...

Creation
--------

# dict of columns / list of row dicts
pd.DataFrame.from_dict(d)

# dict of row dicts
pd.DataFrame.from_dict(d, orient='index')

# set index + index name at time of creation
pd.DataFrame(..., index=pd.Series([...], name='foo'))

# specify column type at time of creation
pd.DataFrame({'foo': pd.Series(['a', 'b', 'c'], dtype='category'), ...})

# specify column type at time of creation (from_dict)
pd.DataFrame.from_dict(..).astype({'foo': 'string'})

# fix column types after creating from dicts, etc.
df = df.astype({"access_count": int, "date_created": "datetime64[ns]"})

# dataframe of random uniform values
df = pd.DataFrame(np.random.rand(3, 4))

# zeros
df = pd.DataFrame(np.zeros((3, 4)))

Saving
------

# move index to column with specified name prior to saving;
# this approach avoids issues if the index is unnamed or a column exists with its name
df = df.reset_index()
df = df.rename(columns={df.columns[0]: "foo"}).to_feather(...)

dates
-----

# convert date column to unix timestamps
df.datetime.values.astype(np.int64) // 10 ** 9

# convert timestamp column to datetime
pd.to_datetime(df['ts'], unit='s')

# get all rows for a specific date
df[df['date'].dt.normalize() == '2021-12-07']

To Dict
-------

# two cols to dict of key:value pairs
dict(zip(x.tag, x.color))

Joins
-----

# inner join by similar column(s)
a.merge(b)
a.merge(b, on=['foo'])

# outer join
a.merge(b, on='col', how='outer')

# join on index
a.join(b)
pd.merge(a, b, left_index=True, right_index=True)

# rbind
pd.concat([a, b])

# cbind
pd.concat([a, b], axis=1)
pd.concat([a, b], axis=1, ignore_index=True)

pd.concat([a, b.set_index(a.index)], axis=1)
pd.concat([a.reset_index(), b.reset_index()], axis=1).set_index("index")
pd.concat([a.reset_index(Drop=True), b.reset_index(Drop=True)], axis=1)

# cbind (numpy)
df = np.concatenate((a,b), axis=1)

Adding & Removing Rows/Cols
---------------------------

# add column at left side
df.insert(0, "foo", [0, 1, 2])
df.insert(0, "foo", [0, 1, 2], allow_duplicates = True)

df['foo'] = 1

# add a "rank" column (1-n)
df['rank'] = list(range(1, df.shape[0] + 1))

Moving cols
-----------

# move column to front
x = df.pop("foo")
df.insert(0, "foo", x)

Replacing values
----------------

# replace a single value
df.loc[0, 'column'] = 'foo'

# replace values in all columns
df.replace({'from': 'to', 'foo': 'bar'})

# replace values in a single column
df.replace({'colname': {'from': 'to', 'foo': 'bar'}})

Filtering & Masking
-------------------

# membership / "includes"
x.isin([..])    # is in 
~x.isin([..])   # not in

pd.Series(['a', 'b', 'c']).isin(df.col)

# get rows where index is "xx" (must use a list, even for single indices..)
df.loc[["xx"]]

# get column as series for all rows matching an index;
# if only a single row matches, then the value for that row/col is returned instead of a Series
df.loc[ind, "col"]

# same thing, but always returns a DataFrame, regardless of the number of matches
df.loc[[ind]][["col"]]

# select a single row by index, and columns by boolean mask
x.loc[id][x.loc[id] == 1]

# get values in list that are present in columns of a dataframe
set(a).intersection(df.columns)

# find indices present in one dataframe but not another
a.index[~a.index.isin(b.index)]

# filter rows by column value membership
df.loc[df['column_name'].isin(some_values)]

# filter columns by name
df.loc[:, ~df.columns.isin(cols_to_remove)]

# subset rows & columns using boolean mask
df.loc[:, col_sums <= 100]
df.loc[row_sums >= 0, col_sums <= 100]

# ~R's where()
np.where(df['foo'] > 1)

# ~R's match()
B.reset_index().set_index('foo').loc[A.foo, 'index'].values

# get indices of series items present in a list
s[s.isin(lst)].index

# contains
df.foo.str.contains('bar', na=False)  # ignore missing vals in column

# endswith
x[x.foo.str.endswith('bar')]

# find indices of string in column
df.foo.str.find('bar')

# get rows corresponding to a string match
 df[df.foo.str.lower().str.find('bar') != -1]

# return boolean series indicating regex presence in column
df.foo.str.find('bar|baz')

# multiple conditions (wrap each in parens, use "&" / "|")
x[(x.foo > 3) & (x.bar < 10)]

# remove duplicates (index not considered)
df[~df.duplicated()]
df.drop_duplicates()

# remove duplicate indices
df[~df.index.duplicated(keep='first')]

Type Conversion
---------------

# string/object -> int/category
df['foo'] = df['foo'].astype('int64')
df['foo'] = df['foo'].astype('category')

# bool -> int (in place)
df.replace({False: 0, True: 1}, inplace=True)

Strings
-------

# create dataframe with a string column
pd.DataFrame({'str_col': pd.Series(lst, dtype='string')})

# memory efficient string type;
# notes (jul 2022): 
# 1. much more minimal functionality, e.g. no string concatenation implemented atm
# 2. can run into "offset overflow" issues when attempting to mask large datasets
pd.read_csv(..., dtype={'foo': 'string[pyarrow]'})

Categories
----------

# drop unused levels
df.foo = df.foo.cat.remove_unused_categories()

Memory
------

# useful for memory used due to column data type & optmizing types
df.memory_usage(deep=True)

Min/Max
-------

# get indices of column with max value for each row (~which.max)
df.idxmax(axis=1)

# get the N largest values in a dataframe overall
df.stack().sort_values(ascending=False).iloc[:n]

# get min of two or more cols
df[['a','b']].min(axis=1)

Clip
---

df.clip(0, 3)

Rank
----

# rank along columns (ascending=True by default, so higher values -> higher ranks)
df.rank(axis=0)

Apply
-----

# ex. row means
df.apply(lambda x: x.mean(), axis=1)

Normalization / Scaling
-----------------------

# min-max scaling
(df - df.min()) / (df.max() - df.min())

# mean-center
df.apply(lambda x: x - x.mean())

# min-max scaling (numpy / scikit)
from sklearn import preprocessing
mat = df.to_numpy()
scaler = preprocessing.MinMaxScaler().fit(mat)
scaled_mat = scaler.transform(mat)

# standardization (numpy / scikit)
scaler = preprocessing.StandardScaler().fit(mat)
scaled_mat = scaler.transform(mat)

# scale so that rows sum to "1"
df.div(df.sum(axis=1), axis=0)

Variance
--------

# along rows
df.var(1)
df.var(1).plot.kde(title='..')
df.var(1).plot.kde(logx=True)

Sort
----

# sort (arrange) by column
df.sort_values('foo')

# sort by column (descending)
df.sort_values('foo', ascending=False)

# sort columns by name
df = df.sort_index(axis=1)

Grouping / Summarization
------------------------

# group by -> sum
df.groupby('foo').sum()

# group by -> count
df.groupby('foo').count()
df.foo.value_counts().sort_values(ascending=False)

# group by -> count (limit to a single column)
df.[['foo', 'bar']].groupby("foo").count().sort_values("bar")

# group by -> count unique
df.groupby('foo').foo.nunique().reset_index()

# group by -> xx
df.groupby('foo').agg(sum)
df.groupby('foo').agg('sum')
df.groupby('foo').agg(lambda x: np.sum(x))

# group by -> sort
df.groupby('foo').apply(pd.DataFrame.sort_values, 'bar')
df.groupby('foo').sort_values('bar')

# group by -> count -> max -> sample
df.groupby(["path", "foo"]).count().max(axis=1).groupby("path").sample(1)

# get first item from each group
df.groupby('foo').first()

# ~ungroup()
df.groupby('foo').first().reset_index()

Sampling
--------

df.sample(10, random_state=321)

Counting
--------

# convert a list to a series of frequencies
pd.Series(['a', 'a', 'b', 'c']).value_counts()

Matrix operations
-----------------

# set diagonal to zero
mat.values[tuple([np.arange(mat.shape[0])] * 2)] = 0

# convert count matrix to a co-occurence matrix
df.T.dot(df)

Reshaping
---------

# df -> 1d ndarray
df.values.ravel()

# df -> 1d ndarray (upper-triangular matrix only, excluding diagonal)
ind = np.triu_indices(5, k=1)
df.values[ind]

# pivot long -> wide
df.pivot_table(index='patient', columns='obs', values='score', fill_value=0)

# pivot long -> wide (binary)
df['presence'] = True
mat = df.pivot_table(index='patient', columns='obs', values='presence', fill_value=False)

# if some values are missing in the wide matrix, these will show up as `NaN`s, before
# being filled with "False" in the example below.. the result is that the type of the wide
# matrix will be floats/"objects" instead of "bool", as expected.. to fix, cast type back to bool:
mat = mat.astype(bool)

Renaming
--------

df.columns = [...]
df.rename(columns={"from": "to", ...})

Printing
--------

df.to_markdown()

Plotting
--------

# density plot for column "score"
import matplotlib.pyplot as plt
ax = df.score.plot.kde(title='...')
plt.show()

ax.figure.savefig('out.png', dpi=300)

Clipboard
---------

pd.read_clipboard(sep='\t')
df.to_clipboard()
df.foo.to_clipboard()

Write
-----

# exclude index
df.to_csv(index=False)

Troubleshooting
---------------

> ValueError: cannot reindex from a duplicate axis

# This means that one of the dataframes being operated on has non-unique index values.
# To check:
df.index.is_unique

> IndexingError: Unalignable boolean Series provided as indexer (index of the boolean
> Series and of the indexed object do not match).

# Series indices don't match; use ndarray instead
series[mask.values]
