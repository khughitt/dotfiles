Basics
------

# rows
axis = 0

# columns
axis = 1

Series
------

# rename series
dat.name = 'foo'

# convert dataframe to a series
pd.Series(dat.values.ravel())

Selection
---------

# get index as an integer
ind = df.index[df["col"] == val].tolist()[0]

# get upper triangular matrix indices
ind = np.triu_indices(5)         # incl. diagonal
ind = np.triu_indices(5, k=1)    # excl. diagonal

# extract values of indices as 1d array
dat.values[ind]

# drop column
dat.drop('col', axis=1)

# drop missing
dat.dropna()

Iteration
---------

# iterate over rows as dicts
dat.to_dict(orient="records")

# iterate over rows
for index, row in df.iterrows():
    ...

Creation
--------

# dict of columns
pd.DataFrame.from_dict(d)

# dict of rows
pd.DataFrame.from_dict(d, orient='index')

# fix column types after creating from dicts, etc.
df = df.astype({"access_count": int, "date_created": "datetime64[ns]"})

# dataframe of random uniform values
df = pd.DataFrame(np.random.rand(3, 4))

# zeros
df = pd.DataFrame(np.zeros((3, 4)))

Dates
-----

# convert date column to unix timestamps
df.datetime.values.astype(np.int64) // 10 ** 9

# convert timestamp column to datetime
pd.to_datetime(df['ts'], unit='s')

# get all rows for a specific date
df[df['date'].dt.normalize() == '2021-12-07']

To Dict
-------

# two cols to dict of key:value pairs
dict(zip(x.tag, x.color))

Joins
-----

# inner join by similar column(s)
a.merge(b)
a.merge(b, on=['foo'])

# join on index
a.join(b)

# rbind
pd.concat([a, b])

# cbind
pd.concat([a, b], axis=1)
pd.concat([a, b.set_index(a.index)], axis=1)
pd.concat([a.reset_index(), b.reset_index()], axis=1).set_index("index")
pd.concat([a.reset_index(Drop=True), b.reset_index(Drop=True)], axis=1)

# cbind (numpy)
df = np.concatenate((a,b), axis=1)

Adding & Removing Rows/Cols
---------------------------

df.insert(0, "foo", [0, 1, 2])
df.insert(0, "foo", [0, 1, 2], allow_duplicates = True)

df['foo'] = 1

# add a "rank" column (1-n)
df['rank'] = list(range(1, df.shape[0] + 1))

Replacing values
----------------

df.loc[0, 'column'] = 'foo'

Filtering & Masking
-------------------

# membership / "includes"
x.isin([..])    # is in 
~x.isin([..])   # not in

pd.Series(['a', 'b', 'c']).isin(df.col)

# get values in list that are present in columns of a dataframe
set(a).intersection(df.columns)

# find indices present in one dataframe but not another
a.index[~a.index.isin(b.index)]

# filter rows by column value membership
df.loc[df['column_name'].isin(some_values)]

# filter columns by name
dat.loc[:, ~dat.columns.isin(cols_to_remove)]

# subset rows & columns using boolean mask
dat.loc[row_sums >= 0, col_sums <= 100]

# ~R's where()
np.where(dat['foo'] > 1)

# ~R's match()
B.reset_index().set_index('c').loc[A.c, 'index'].values

# contains
df.foo.str.contains('bar', na=False)  # ignore missing vals in column

# endswith
x[x.foo.str.endswith('bar')]

# find indices of string in column
df.foo.str.find('bar')

# get rows corresponding to a string match
 dat[dat.foo.str.lower().str.find('bar') != -1]

# return boolean series indicating regex presence in column
df.foo.str.find('bar|baz')

# multiple conditions (wrap each in parens, use "&" / "|")
x[(x.foo > 3) & (x.bar < 10)]

# remove duplicates
df[~df.duplicated()]

Type Conversion
---------------

# string/object -> int
df['foo'] = df['foo'].astype('int64')

# bool -> int (in place)
df.replace({False: 0, True: 1}, inplace=True)

Min/Max
-------

# get indices of column with max value for each row (~which.max)
df.idxmax(axis=1)

# get the N largest values in a dataframe overall
df.stack().sort_values(ascending=False).iloc[:n]

Normalization / Scaling
-----------------------

# min-max scaling
(df - df.min()) / (df.max() - df.min())

# mean-center
df.apply(lambda x: x - x.mean())

# min-max scaling (numpy / scikit)
from sklearn import preprocessing
mat = df.to_numpy()
scaler = preprocessing.MinMaxScaler().fit(mat)
scaled_mat = scaler.transform(mat)

# standardization (numpy / scikit)
scaler = preprocessing.StandardScaler().fit(mat)
scaled_mat = scaler.transform(mat)

Variance
--------

# along rows
df.var(1)
df.var(1).plot.kde()
df.var(1).plot.kde(logx=True)

Sort
----

# sort by column
dat.sort_values('foo')

# sort by column (descending)
dat.sort_values('foo', ascending=False)

Grouping / Summarization
------------------------

# group by -> sum
dat.groupby('tag').agg(sum)

# group by -> count
dat.groupby('tag').agg('count')

# group by -> count unique
dat.groupby('tag').foo.nunique().reset_index()

# group by -> sort
dat.groupby('seed').apply(pd.DataFrame.sort_values, 'tag')
dat.groupby('seed').sort_values('tag')

# group by -> count -> max -> sample
dat.groupby(["path", "seed"]).count().max(axis=1).groupby("path").sample(1)

# get first item from each group
df.groupby('foo').first()

Sampling
--------

df.sample(10, random_state=321)

Counting
--------

# convert a list to a series of frequencies
pd.Series(['a', 'a', 'b', 'c']).value_counts()

Matrix operations
-----------------

# set diagonal to zero
mat.values[tuple([np.arange(mat.shape[0])] * 2)] = 0

# convert count matrix to a co-occurence matrix
a.T.dot(a)

Reshaping
---------

# dataframe -> 1d ndarray
dat.values.ravel()

# dataframe -> 1d ndarray (upper-triangular matrix only, excluding diagonal)
ind = np.triu_indices(5, k=1)
dat.values[ind]

# pivot long -> wide
df.pivot_table(index='patient', columns='obs', values='score', fill_value=0)

# pivot long -> wide (binary)
df['presence'] = 1
df.pivot_table(index='patient', columns='obs', values='presence', fill_value=0)

Renaming
--------

df.columns = [...]
df.rename(columns={"from": "to", ...})

Printing
--------

df.to_markdown()

Plotting
--------

# density plot for column "score"
import matplotlib.pyplot as plt
ax = dat.score.plot.kde()
plt.show()

Write
-----

# exclude index
dat.to_csv(index=False)

