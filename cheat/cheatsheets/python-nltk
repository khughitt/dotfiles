# tokenization (nltk)
nltk.tokenize.RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+')
tokenizer.tokenize(doc)

# lemmatization (nltk)
from nltk.stem.wordnet import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemmatizer.lemmatize(word)

# stemmer comparison
# https://stackoverflow.com/a/24663617/554531
