#
# pyarrow
# https://arrow.apache.org/docs/python/getstarted.html
#

# arrays & tables
import pyarrow as pa

a = pa.array([1, 2, 3], type=pa.int8())
b = pa.array([

tbl = pa.table([a, b], names=['col1' 'col2'])

# dataset
import pyarrow.dataset as ds

# data types (https://arrow.apache.org/docs/python/api/datatypes.html)
pa.bool_()
pa.int8()
pa.int16()
pa.int32()
pa.int64()
pa.uint8(), .., pa.uint64()
float16(), .., pa.float64()
date32/64()
time32/64()
timestamp()
string()
large_string()
list_()
map_()
struct()
dictionary()
field()
utf8()
binary()

# parquet (default: snappy)
import pyarrow.parquet as pq

pq.read_table('in.parquet')
pq.write_table(tbl, 'out.parquet')

# feather (default: lz4)
import pyarrow.feather as ft

ft.read_table('in.feather')
ft.write_feather(tbl, 'out.feather', compression='lz4')

# pandas x series
arr.to_pandas()
pa.Array.from_pandas(s)

# pandas x dataframes
tbl.to_pandas()
pa.Table.from_pandas(df)
pa.Schema.from_pandas(df)

# combine multiple dataframes into a single parquet file (more memory-efficient than pd.concat)
from pyarrow.parquet import ParquetWriter

tbl = pa.Table.from_pandas(first_df)
pqwriter = ParquetWriter('out.parquet', tbl.schema)
pqwriter.write_table(tbl)

for infile in remaining_files:
    tbl = pa.Table.from_pandas(pd.read_feather(infile))
    pqwriter.write_table(tbl)

pqwriter.close()

# streaming
# https://wesmckinney.com/blog/arrow-streaming-columnar/
batch = pa.RecordBatch.from_pandas(df)

sink = pa.InMemoryOutputStream()
stream_writer = pa.StreamWriter(sink, batch.schema)

# vi:syntax=python
